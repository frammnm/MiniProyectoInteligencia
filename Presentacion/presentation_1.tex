%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Beamer Presentation
% LaTeX Template
% Version 1.0 (10/11/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND THEMES
%----------------------------------------------------------------------------------------

\documentclass{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
% \usetheme{AnnArbor}
\usetheme{Antibes}
%\usetheme{Bergen}
%\usetheme{Berkeley}
%\usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
%\usetheme{Copenhagen}
%\usetheme{Darmstadt}
%\usetheme{Dresden}
%\usetheme{Frankfurt}
%\usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
%\usetheme{Madrid}
%\usetheme{Malmoe}
%\usetheme{Marburg}
%% \usetheme{Montpellier}
%\usetheme{PaloAlto}
%\usetheme{Pittsburgh}
%%\usetheme{Rochester}
% \usetheme{Singapore}
% \usetheme{Szeged}
% \usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

%\usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
%\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

%\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title[Proyecto Final]{Clasificador de crimenes} % The short title appears at the bottom of every slide, the full title is only on the title page

\author{Gabriel \'{A}lvarez 09-10029 - Francisco Mart\'{i}nez 09-10502 - Prof. Masun Nabhan Homsi} % Your name
\institute[USB] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
Universidad Sim\'{o}n Bol\'{i}var \\ % Your institution for the title page
\medskip
\textit{gabrielaar11@gmail.com - frammnm@gmail.com - mnabhan@usb.ve} % Your email address
}
\date{\today} % Date, can be changed to a custom date

\begin{document}    

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}[allowframebreaks=0.95]
\frametitle{Tabla de contenido} % Table of contents slide, comment this block out to remove it
\tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
\end{frame}

%----------------------------------------------------------------------------------------
%	PRESENTATION SLIDES
%----------------------------------------------------------------------------------------

%------------------------------------------------
\section{Introducci\'{o}n} % Sections can be created in order to organize your presentation into discrete blocks, all sections and subsections are automatically printed in the table of contents as an overview of the talk
%------------------------------------------------
\begin{frame}
\frametitle{Introducci\'{o}n}
Este mini-proyecto pretende desarrollar una aplicaci\'{o}n m\'{o}vil para visualizar en forma gráfica los eventos ocurridos referentes a crimen en la ciudad de Caracas. En este documento explicaremos el trabajo realizado para extraer los datos que vienen como texto en forma de tweets utilizando el API de Twitter, el preprocesamiento realizado para la obtención de términos relevantes y representantes del tema de crimen como también los pasos, algoritmos y tareas necesarias para entrenar los algoritmos para la correcta clasificación de los tweets de manera tal que se puedan identificar tweets nuevos y en tiempo real acerca del tema. Finalmente se hablarán de los resultados encontrados, los cuales consideramos como suficientemente exitosos para seguir en los siguientes pasos a trabajar, que serán la subclasificación de los tweets en tipos de crimen ( asesinato, robo, pelea, etc.), la visualización gráfica de los mismos en un mapa digital de Caracas y finalmente la conglomeración de todas las partes envueltas en una aplicación móvil que pueda ser utilizada de forma masiva para así brindar beneficios a la comunidad e instituciones que puedan utilizar estos datos para un fin. 
\end{frame}
%------------------------------------------------
\section{Descripci\'{o}n del Problema} 
\begin{frame}
\frametitle{Descripci\'{o}n del Problema}
El problema a trabajar posee varios subtareas que deben ser resueltas para el cumplimiento de los objetivos: 
\item La obtención de datos.
\item El preprocesamiento del texto.
\item La clasificación de los tweets.

\end{frame}
%------------------------------------------------
\section{Objetivos} 

\subsection{Objetivo General}
\begin{frame}
\frametitle{Objetivo General}
\begin{itemize}
\item Diseñar e implementar una aplicación móvil para visualizar los eventos ocurridos en la ciudad de Caracas
\end{itemize}
\end{frame}

\subsection{Objetivos Especificos}
\begin{frame}
\frametitle{Objetivos Especificos}
\begin{itemize}
\item Aplicar los algoritmos de procesamiento de lenguaje natural para la limpieza del texto fuente
(Twitters).
\item Desarrollar la base de datos para llevar el control de los usuarios de la aplicación.
\item Analizar y procesar el texto para detectar patrones relacionados a eventos.
\item Usar algoritmos avanzados de visualización de datos.
\item Modelar el usuario para brindarle una lista de eventos cercanas a su interés.
\item Hallar visualmente las relaciones entre diferentes eventos.
\item Implementar la interfaz del sistema.
\item Presentar el sistema a través de una interfaz web amigable para el usuario.
\end{itemize}

\end{frame}
%------------------------------------------------



\section{Metodologia}

\subsection{Obtencion de datos}
%------------------------------------------------
\begin{frame}
\frametitle{Obtencion de datos}
La obtención de los datos fue utilizando el API de twitter y el lenguaje de programación Python, a través del API de twitter se pudo especificar las cuentas de las cuales se iban a obtener los tweets y la cantidad de tweets que se iban a utilizar, alrededor de 33000 tweets. Estas cuentas fueron las que más relacionadas estuviesen con los crímenes en el área de Caracas.

\item La libreria tweepy
\end{frame}
%------------------------------------------------


\subsection{Preprocesamiento}
%------------------------------------------------
\begin{frame}
\frametitle{Preprocesamiento}
El preprocesamiento consistió en eliminar los signos de puntuación de los tweets, menciones, signos de exclamación, signos de interrogación, corchetes, parentesis y links. También se transformó todo el texto a minúscula. Este filtro fue realizado debido a que estos elementos no presentaban información relevante para los crímenes. Luego, se utilizó la herramienta WEKA, la cual posee una variedad de opciones para el procesamiento del lenguaje natural, así como algoritmos de clasificación. En este caso, los pasos, de forma general, a seguir fueron los siguientes:


Se cargó el archivo de los tweets, con el formato de WEKA. Este archivo en un principio se le colocó de manera aleatoria las etiquetas de yes/no.
Una vez cargado el conjunto de datos, se procedió a obtener los unigramas, bigramas y trigramas. Utilizando las opciones de IDF y TF, junto con stopwords y stemming para el caso de los unigramas.
Luego de obtener los resultados, se procedió a filtrar manualmente los mismos. Dejando sólo los unigramas, bigramas y trigramas que más relacionados estuviesen con crímenes.
Una vez obtenida esta información, así como unas palabras extras ofrecidas por la profesora, se procedió a etiquetar correctamente el conjunto de datos.
\end{frame}
%------------------------------------------------


\subsubsection{Stopwords}
%------------------------------------------------
\begin{frame}
\frametitle{Stopwords}
Los stopwords son las palabras más comunes dentro del lenguaje, generalmente los artículos, estas palabras fueron eliminadas para la obtención de los unigramas ya que no aportan información. En este caso, se utilizó la opción de “WordsFromFile” de WEKA. En esta opción se le debe proporcionar un archivo con la lista de palabras que serán utilizadas para la eliminación de stopwords, en nuestro caso se utilizó una lista de stopwords en español. 

\end{frame}
%------------------------------------------------


\subsubsection{N-Grams para determinar conceptos relevantes}
%------------------------------------------------
\begin{frame}
\frametitle{Determinacion para conceptos relevantes (NGram)}
Para determinar los términos relevantes para el tema de crimen, utilizando la herramienta Weka se uso un proceso el cual dado un texto ya libre de datos irrelevantes como signos de puntuación, enlaces, numeros, llaves, corchetes, signos matemáticos, referencias a otras cuentas de Twitter, saltos de línea, y otros caracteres que no brindan información en lo que respecta a un  tema ( una coma por ejemplo no me dice si un tema es de crimen, o no, al igual que los otros elementos mencionados),  se divide y se crean gramas de tamaños específicos. Un grama viene siendo un conjunto de tamaño específico de palabras consecutivas en el texto, para que quede de forma clara se ejemplifica a continuación : 


Dado el texto : “ El asesinato en las Mercedes” 


Dependiendo del tamaño buscado se pueden generar distintas listas de gramas:
 
Unigramas: (“El”, ”asesinato”, ”en”, “las”, “Mercedes”) 
Bigramas : (“El asesinato”, “asesinato en”, “en las” ,”las Mercedes”) 
Trigramas : (“El asesinato en”, “asesinato en las”, “en las Mercedes”) 


Se puede observar como el n-grama es la aglomeración de palabras que pueden representar conceptos donde además pueden ser de cualquier tamaño y aunque se ejemplifico hasta el trigrama, es posible sacar tetragramas, pentagramas y en general cualquier n-grama, pero para nuestro proyecto se decidió sacar desde unigramas hasta trigramas, para así dar prioridad a los conceptos que sean representados con pocas palabras ya que al tratarse de una búsqueda en tweets la brevedad es fundamental al momento de expresarse. En el caso de los unigramas sacados se eliminaron los stopwords como ya fue mencionado, pero para el caso de bigramas y trigramas este no fue el caso, ya que es sumamente común conectar palabras relevantes a través de artículos y palabras auxiliares.
 
Al obtener la lista de los distintos gramas cada lista fue filtrada de conceptos irrelevantes, para explicar cuáles de estos términos fueron los eliminados hay que poner en contexto que las cuentas de Twitter utilizadas no hablan únicamente de crímenes, ya que también pueden hablar de noticias económicas, tecnológicas, culturales, y sobre todo un tema recurrente es de la política, términos como Mesa de la Unidad, Capriles, biotecnología, felicidad, entre otros, que terminan siendo irrelevantes para nuestros objetivos, asi que cada lista fue filtrada por separado para finalmente tener un solo conjunto de términos resultantes que provienen de la unión de estos distintos conjuntos de gramas.
 
Ahora es necesario explicar como un grama dado llega a estar en la listas de gramas creadas, para esto hay que entender que no todas las palabras de la lengua española se utilizan con la misma frecuencia (como en la mayoría de los idiomas), ya que palabras como “la”, “el”, “del” entre otras son usadas de manera sumamente frecuente, a diferencia de palabras clave como “crimen”, “asesinato”, “arma”, y por esto no es suficiente que la lista de de gramas sea basado únicamente en la frecuencia de aparición de las palabras en el texto, para resolver este problema se realiza una normalización de la frecuencia que toma en cuenta que tan frecuente es esa palabra en el texto para así darle más importancia a palabras relevantes en vez de articulos y conectores, dicha transformación se llama TF-IDF (term frequency–inverse document frequency) , en la cual se toma en cuenta la frecuencia de aparición de las palabras pero dándole un poco mas de peso a las palabras menos frecuente, como puede ser la palabra “robo” en vez de palabras como “en”, o “del”, esto nos permite crear listas que posean gramas que representen conceptos relevantes en esas cuentas de Twitter, para luego filtrar los temas no deseados y asi obtener como producto final los terminos relevantes a nuestro tema especifico.
\end{frame}
%------------------------------------------------


\subsubsection{Etiquetacion de tweets}
%------------------------------------------------
\begin{frame}
\frametitle{Etiquetacion de tweets}
Para etiquetar los tweets, se utilizaron los términos más relevantes, obtenidos en el punto anterior. El proceso utilizado fue el siguiente: De los unigramas, bigramas y trigramas obtenidos, se generaron tres listas, en una de ellas se guardaron los términos que mejor explicaran qué sucedió, en otra se guardaron los términos que tuviesen que ver con el tiempo y en la última los términos que mejor explicaran el cómo sucedió, Estas listas fueron denominadas como “What”, “When” y “How”, respectivamente. Luego para etiquetar los tweets se utilizaron la lista de “What” y “How”, la lista “When” no fue utilizada porque por sí sola no aportaba información referente a crímenes; sí los tweets poseían términos de alguna de estas dos listas, entonces eran considerados como tweets de crimen y eran etiquetados con un “Yes”, en caso contrario se etiquetaban con un “No”. 

\end{frame}
%------------------------------------------------


\subsubsection{TF-IDF}
%------------------------------------------------
\begin{frame}
\frametitle{TF-IDF}
En la obtención de información tf–idf ( term frequency–inverse document frequency), es un valor estadístico que busca reflejar la importancia de un término en un texto dado,  esta normalización busca darle más peso a aquellas palabras que son usadas de manera poco común, y castigar a aquellas que son utilizadas de manera extensiva. Por ejemplo en la frase “la casa azul” quiere ser buscado en un conjunto de documentos y devolver el documento que sea más relevante en relación a esa frase, directamente se podría contar la cantidad de frecuencia de cada término, y devolver el documento que tenga el mayor valor para la suma de estas frecuencias, pero esto devolverá documentos que utilicen más intensivamente el término “la” en vez de los terminos más relevantes “casa” y “azul”, esto sucede ya que la mayoría de los terminos más repetidos llegan a ser articulos y conectores en vez de palabras de conceptos más complicados, asi que esta transformación ayuda obtener los documentos más relevantes para la frase castigando el término “la” y premiando a “casa” y “azul”. Matemáticamente, TF-IDF es el producto de la frecuencia del término y la frecuencia inversa del término, la frecuencia inversa del término viene siendo el logaritmo de la cantidad de documentos entre la cantidad de documentos que poseen el término.
 
Esta técnica fue la utilizada para obtener los terminos relevantes a través de los miles de tweets que se poseen como datos en el preprocesamiento del texto. 
 
Para apreciar mas en detalle el hecho de una mayor frecuencia de ciertos terminos en los lenguajes naturales  revisar la Ley de Zipf.
\end{frame}
%------------------------------------------------




\subsection{Clasificacion de los tweets}
\subsubsection{KNN}
%------------------------------------------------
\begin{frame}
\frametitle{KNN}
De las siglas en inglés k-nearest neighbors, este algoritmo en la fase de entrenamiento consiste en colocar todos los vectores del conjunto de entrenamiento en el espacio con su respectiva clase y para clasificar un nuevo vector el proceso es el siguiente: se obtienen los k vecinos más cercanos a ese vector, y el vector es etiquetado con la clase que más aparezca en sus k vecinos. El proceso para determinar los k vecinos varía en la implementación, aunque comúnmente se utiliza la distancia euclidiana para el caso de variables continuas y la distancia de Hamming para las variables discretas. El algoritmo utilizado fue el de IBK de la herramienta de WEKA, el mejor resultado obtenido fue de 73% de ejemplos clasificados correctamente.
\end{frame}
%------------------------------------------------
\subsubsection{Arbol de decision(j48)}
%------------------------------------------------
\begin{frame}
\frametitle{Arbol de decision(j48)}
En el algoritmo C4.5 se genera un árbol utilizando la entropía de la información, en cada nodo del árbol de decisión el algoritmo utiliza el atributo que mejor divide el conjunto de entrenamiento en subconjuntos de clases diferentes. Finalmente para clasificar, sólo se debe seguir el árbol de decisión generado.
\end{frame}
%------------------------------------------------
\subsubsection{Convolution NN(Deep Learning)}
%------------------------------------------------
\begin{frame}
\frametitle{Convolution NN(Deep Learning)}
Este algoritmo funciona similar a una red neuronal donde alimenta datos a través de las capas y tiene como objetivo reducir el error del resultado calculado en la red a través de cada iteración, a diferencia de sus contrapartes las redes neuronales convolucionales tienen un arreglo distinto de las neuronas, donde las capas de neuronas se llaman campos receptivos donde distintas capas poseen trabajos diferentes, siendo una capa convolucional que se encarga de extraer las caracteristicas, otra que se ocupa de la reducción de muestreo que sirve para generalizar características de manera tal que haya una tolerancia a ruido en los datos y finalmente una capa de clasificación que se encarga de poner la muestra en su etiqueta correcta luego de la depuración de la información. Aunque este algoritmo tiene aplicaciones más que todo gráficas, se ha demostrado que tiene buenos resultados para el procesamiento de lenguaje natural, para el análisis de juego (Go), descubrimiento de drogas entre otras aplicaciones que se alejan del campo de imagenes.
\end{frame}
%------------------------------------------------
\subsection{Evaluacion}
\subsubsection{Matriz de confusion}
%------------------------------------------------
\begin{frame}
\frametitle{Matriz de confusion}
\item KNN (IBK con Unigramas,Bigramas y Trigramas)

\begin{table}
\begin{center}
\begin{tabular}{ | p{2 cm} | p{2 cm} | }
\toprule
\textbf{No} & \textbf{Yes}\\
\midrule
1906.25 & 98.83\\ \hline
988.93 & 1042.84\\ \hline
\bottomrule
\end{tabular}
\end{center}
\end{table}

\item C4.5 (J48 con Unigramas, Bigramas y Trigramas)

\begin{table}
\begin{center}
\begin{tabular}{ | p{2 cm} | p{2 cm} | }
\toprule
\textbf{No} & \textbf{Yes}\\
\midrule
1959.57 & 45.51\\ \hline
133.09 & 1898.68\\ \hline
\bottomrule
\end{tabular}
\end{center}
\end{table}

\item CNN (con Unigramas, Bigramas y Trigramas)

\begin{table}
\begin{center}
\begin{tabular}{ | p{2 cm} | p{2 cm} | }
\toprule
\textbf{No} & \textbf{Yes}\\
\midrule
1960.28 & 44.79\\ \hline
161.73 & 1870.04\\ \hline
\bottomrule
\end{tabular}
\end{center}
\end{table}

\end{frame}
%------------------------------------------------
\subsubsection{Sensibilidad(Recall)}
%------------------------------------------------
\begin{frame}
\frametitle{Sensibilidad}
\item KNN (IBK con Unigramas, Bigramas y Trigramas): 0.731
\item C4.5 (J48 con Unigramas, Bigramas y Trigramas): 0.956
\item CNN (con Unigramas, Bigramas y Trigramas): 0.949 
\end{frame}
%------------------------------------------------
\subsubsection{Especificidad}
%------------------------------------------------
\begin{frame}
\frametitle{Especificidad}
\item KNN (IBK con Unigramas, Bigramas y Trigramas): 0.787
\item C4.5 (J48 con Unigramas, Bigramas y Trigramas): 0.957
\item CNN (con Unigramas, Bigramas y Trigramas): 0.950
\end{frame}
%------------------------------------------------
\subsubsection{F-Score}
%------------------------------------------------
\begin{frame}
\frametitle{F-Score}
\item KNN (IBK con Unigramas,Bigramas y Trigramas): 0.717
\item C4.5 (J48 con Unigramas, Bigramas y Trigramas): 0.956
\item CNN (con Unigramas, Bigramas y Trigramas):0.949
\end{frame}
%------------------------------------------------



\section{Resultados}


\subsection{Resultados del preprocesamiento}
%------------------------------------------------
\begin{frame}
\frametitle{Resultados del preprocesamiento}
\end{frame}
%------------------------------------------------


\subsection{Resultados de la clasificacion}
%------------------------------------------------
\begin{frame}
\frametitle{Resultados de la clasificacion}
\end{frame}
%------------------------------------------------




\section{Conclusiones}

%------------------------------------------------
\begin{frame}
\frametitle{Conclusiones}
Concluimos que los resultados obtenidos son lo suficientemente buenos para ser utilizados en una aplicación de uso masivo, siendo estos consistentes desde el punto de vista de aprendizaje de máquina, y que estamos preparados para los siguientes pasos los cuales se discutirán en el siguiente punto de este documento, pero de forma general se pueden listar como el desarrollo de la aplicación, una definición de los usuarios, el trabajo de enlazar el modelo de inteligencia artificial en la aplicación, y la graficación de la data. 
Es relevante recalcar que el proceso de aprendizaje de máquina aún no se ha terminado, pues la subclasificación de los crímenes por tipo, tipo de armas utilizadas, lugar del evento al igual como el momento del evento son objetivos futuros que se desean ampliar y trabajar para ser reflejados en una experiencia para el usuario más rica y enriquecedora. 
También es bueno mencionar que nos sentimos satisfechos de lograr observar los frutos de esta subdisciplina de la ciencia de la computación, ya que trabajar con ideas, algoritmos, herramientas y personas en el campo nos hace apreciar de las capacidades del área, ayudándonos a comprender cómo podemos apoyar a la sociedad a tomar decisiones más inteligentes, estar mas protegidos, y utilizar la información de forma provechosa. 
\end{frame}
%------------------------------------------------





\section{Trabajos Futuros}
%------------------------------------------------
\begin{frame}
\frametitle{Trabajos Futuros}
\end{frame}
%------------------------------------------------
% \begin{table}
% \begin{center}
% \begin{tabular}{ | p{2 cm} | p{2 cm} | p{2 cm} | p{2 cm} | p{2 cm} | }
% \toprule
% \textbf{M\'{e}todo} & \textbf{Predicci\'{o}n} & \textbf{ P / R } & \textbf{Precisi\'{o}n(P)} & \textbf{Recall(R)}\\
% \midrule
% RF & 0.678 & 0.676 & 0.675 & 0.678\\ \hline
% SVM & 0.722 & 0.730 & 0.783 & 0.722\\ \hline
% NB & 0.898 & 0.898 & 0.898  & 0.898\\ \hline 
% CT & 0.860 & 0.860 & 0.861 & 0.817\\ \hline
% kNN & 0.817 & 0.816 & 0.816 & 0.817\\ \hline 
% LR & 0.973 & 0.973 & 0.973 & 0.973\\ 
% \bottomrule
% \end{tabular}
% \end{center}
% \end{table}
%------------------------------------------------


% \begin{frame}
% \frametitle{Bullet Points}
% \begin{itemize}
% \item Lorem ipsum dolor sit amet, consectetur adipiscing elit
% \item Aliquam blandit faucibus nisi, sit amet dapibus enim tempus eu
% \item Nulla commodo, erat quis gravida posuere, elit lacus lobortis est, quis porttitor odio mauris at libero
% \item Nam cursus est eget velit posuere pellentesque
% \item Vestibulum faucibus velit a augue condimentum quis convallis nulla gravida
% \end{itemize}
% \end{frame}

%------------------------------------------------

% \begin{frame}
% \frametitle{Blocks of Highlighted Text}
% \begin{block}{Block 1}
% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.
% \end{block}

% \begin{block}{Block 2}
% Pellentesque sed tellus purus. Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Vestibulum quis magna at risus dictum tempor eu vitae velit.
% \end{block}

% \begin{block}{Block 3}
% Suspendisse tincidunt sagittis gravida. Curabitur condimentum, enim sed venenatis rutrum, ipsum neque consectetur orci, sed blandit justo nisi ac lacus.
% \end{block}
% \end{frame}

%------------------------------------------------

% \begin{frame}
% \frametitle{Multiple Columns}
% \begin{columns}[c] % The "c" option specifies centered vertical alignment while the "t" option is used for top vertical alignment

% \column{.45\textwidth} % Left column and width
% \textbf{Heading}
% \begin{enumerate}
% \item Statement
% \item Explanation
% \item Example
% \end{enumerate}

% \column{.5\textwidth} % Right column and width
% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer lectus nisl, ultricies in feugiat rutrum, porttitor sit amet augue. Aliquam ut tortor mauris. Sed volutpat ante purus, quis accumsan dolor.

% \end{columns}
% \end{frame}

%------------------------------------------------
% \section{Second Section}
%------------------------------------------------

% \begin{frame}
% \frametitle{Table}
% \begin{table}
% \begin{tabular}{l l l}
% \toprule
% \textbf{Treatments} & \textbf{Response 1} & \textbf{Response 2}\\
% \midrule
% Treatment 1 & 0.0003262 & 0.562 \\
% Treatment 2 & 0.0015681 & 0.910 \\
% Treatment 3 & 0.0009271 & 0.296 \\
% \bottomrule
% \end{tabular}
% \caption{Table caption}
% \end{table}
% \end{frame}

% %------------------------------------------------

% \begin{frame}
% \frametitle{Theorem}
% \begin{theorem}[Mass--energy equivalence]
% $E = mc^2$
% \end{theorem}
% \end{frame}

% %------------------------------------------------

% \begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
% \frametitle{Verbatim}
% \begin{example}[Theorem Slide Code]
% \begin{verbatim}
% \begin{frame}
% \frametitle{Theorem}
% \begin{theorem}[Mass--energy equivalence]
% $E = mc^2$
% \end{theorem}
% \end{frame}\end{verbatim}
% \end{example}
% \end{frame}

% %------------------------------------------------

% \begin{frame}
% \frametitle{Figure}
% Uncomment the code on this slide to include your own image from the same directory as the template .TeX file.
% %\begin{figure}
% %\includegraphics[width=0.8\linewidth]{test}
% %\end{figure}
% \end{frame}

% %------------------------------------------------

% \begin{frame}[fragile] % Need to use the fragile option when verbatim is used in the slide
% \frametitle{Citation}
% An example of the \verb|\cite| command to cite within the presentation:\\~

% This statement requires citation \cite{p1}.
% \end{frame}

%------------------------------------------------

% \begin{frame}
% \frametitle{References}
% \footnotesize{
% \begin{thebibliography}{99} % Beamer does not support BibTeX so references must be inserted manually as below
% \bibitem[Smith, 2012]{p1} John Smith (2012)
% \newblock Title of the publication
% \newblock \emph{Journal Name} 12(3), 45 -- 678.
% \end{thebibliography}
% }
% \end{frame}

%------------------------------------------------

\begin{frame}
\Huge{\centerline{Final de la presentaci\'{o}n}}
\end{frame}

%----------------------------------------------------------------------------------------

\end{document} 